demonitization_data <-read.csv("C:\\Users\\vibhor.5.taneja\\Desktop\\hackathon\\Data\\demonetization-tweets.csv")
library(ggplot2) # Libarary for Data visualization
library(readr) # library to read CSV file
library(syuzhet) #library for sentimental analysis
library(tm)
library(wordcloud)
library(dplyr)
library(tidytext)
library(tidyr)
library(igraph)
library(ggraph)
library(wordcloud2)
######################### Sentiments according to predefined library ########################################
demonitization_data_text<-as.character(demonitization_data$text)
filter_text<-gsub("(RT|via)((?:\\b\\w*@\\w+)+)","",demonitization_data_text) #removing Retweets
filter_text<-gsub("http[^[:blank:]]+","",filter_text) #clean html links
filter_text<-gsub("@\\w+","",filter_text)# removing people names
filter_text<-gsub("[[:punct:]]"," ",filter_text)#removing punctuations
filter_text<-gsub("[^[:alnum:]]"," ",filter_text)#removing number (alphanumeric)
sentiment_twitter<-get_nrc_sentiment((filter_text)) #sentimental analysis
Sentimentscores<-data.frame(colSums(sentiment_twitter[,])) #used to classify sentiment scores
library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function
library(tm)
library(wordcloud)
library(plyr)
library(lubridate)
library(syuzhet)
#Import the twitter data set
demonitization_data=read.csv('C:\\Users\\vibhor.5.taneja\\Desktop\\hackathon\\Data\\demonetization-tweets-2.csv',stringsAsFactors = FALSE)
options(warn=-1)
summary(demonitization_data)
demonitization_data$created_date=as.Date(demonitization_data$created,format='%Y-%m-%d %H:%M:%S')#convert created to date format
demonitization_data$hour = format(as.POSIXct(demonitization_data$created,format="%Y-%m-%d %H:%M:%S"),"%H")#Extract Hour from the date
demonitization_data$isRetweetNum=ifelse(demonitization_data$isRetweet==FALSE,0,1)#Numerical variable to indicate whether a tweet was retweet
demonitization_data$retweetedNum=ifelse(demonitization_data$retweeted==FALSE,0,1)#Total number of times a tweet was tetweeted
demonitization_data$tweet=c(1)#Additional column that will help us in summing up total tweets
options(repr.plot.width=6, repr.plot.height=4)
HourFrame=as.data.frame(table(demonitization_data$hour))
colnames(HourFrame)=c("Hour","TweetCount")
HourFrame$Hour=as.numeric(HourFrame$Hour)
y=ddply(demonitization_data, .(demonitization_data$hour), numcolwise(sum))
HourFrame$retweetedNum=y$isRetweetNum
ggplot(HourFrame,aes(x=Hour))+geom_line(aes(y = TweetCount, colour = "TotalTweets")) +
geom_line(aes(y = retweetedNum, colour = "Retweets"))
################################## device data ###########################################
devices=demonitization_data$statusSource
devices <- gsub("","", devices)
devices <- strsplit(devices, ">")
devices <- sapply(devices,function(x) ifelse(length(x) > 1, x[2], x[1]))
devices_source=as.data.frame(table(devices))
colnames(devices_source)=c("Device","TweetCount")
devices_source=devices_source[devices_source$TweetCount>50,]
devices_source=devices_source[order(-devices_source$TweetCount),]
ggplot(devices_source,aes(x=reorder(Device, -TweetCount),y=TweetCount,fill=TweetCount))+geom_bar(stat='identity') +coord_flip()
################ Most Popular users ######################################################
y=ddply(demonitization_data, .(screenName), numcolwise(sum))
popularUsers=y[,c("screenName","retweetCount","tweet")]
popularUsers=popularUsers[order(-popularUsers$retweetCount),]
popularUsers=head(popularUsers,n=10)
popularUsers
################ Most Replies ###############################################################
Replies=demonitization_data[is.na(demonitization_data$replyToSN)==FALSE,]
y=ddply(Replies, .(replyToSN), numcolwise(sum))
Replies=y[,c("replyToSN","tweet")]
Replies=Replies[order(-Replies$tweet),]
Replies=head(Replies,n=20)
colnames(Replies)=c("User","RepliesReceived")
Replies
library(ggplot2) # Libarary for Data visualization
library(readr) # library to read CSV file
library(syuzhet) #library for sentimental analysis
library(tm)
library(wordcloud)
library(dplyr)
library(tidytext)
library(tidyr)
library(igraph)
library(ggraph)
library(wordcloud2)
demonitization_data <-read.csv("C:\\Users\\vibhor.5.taneja\\Desktop\\hackathon\\Data\\demonetization-tweets.csv")
demonitization_bigrams <- unnest_tokens(demonitization_data, input = text, output = bigram, token = "ngrams", n=2)
demonitization_bigrams %>% count(bigram, sort = TRUE)
demonitization_bigrams_separated <- demonitization_bigrams %>% separate(bigram, c("word1", "word2"), sep = " ")
demonitization_bigrams_filtered <- demonitization_bigrams_separated %>% filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word)
# new bigram counts:
demonitization_bigrams_counts <- demonitization_bigrams_filtered %>% count(word1, word2, sort = TRUE)
demonitization_bigrams_counts
AFINN <- get_sentiments("afinn")
not_words <- demonitization_bigrams_separated %>% filter(word1 == "not") %>% inner_join(AFINN, by = c(word2 = "word")) %>%
count(word2, score, sort = TRUE) %>% ungroup()
not_words %>% mutate(contribution = n * score) %>% arrange(desc(abs(contribution))) %>%
head(20) %>% mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(word2, n * score, fill = n * score > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by \"not\"") +
ylab("Sentiment score * number of occurrences") +
coord_flip()
library(tidyverse)
library(boot)
stackOverflow <- read_csv("C:\\Users\\vibhor.5.taneja\\Desktop\\hackathon\\Data\\survey_results_public.csv")
# Looking at the vars in question:
head(stackOverflow)
summary(stackOverflow$Salary)
summary(stackOverflow$JobSatisfaction)
library(tidyverse)
library(boot)
stackOverflow <- read_csv("C:\\Users\\vibhor.5.taneja\\Desktop\\hackathon\\Data\\survey_results_public.csv")
# Looking at the vars in question:
head(stackOverflow)
summary(stackOverflow$Salary)
summary(stackOverflow$JobSatisfaction)
has_salary <- stackOverflow %>%
filter(Salary != "NA") %>%
filter(JobSatisfaction != "NA")
dim(has_salary)
unique(has_salary$JobSatisfaction)   # just a quick head check
# linear model to predict Job Satisfaction by Salary
model <- glm(JobSatisfaction ~ Salary, data = has_salary , family = "poisson")
model
glm.diag.plots(model)
# log(0) is never a good idea.
has_salary2 <- has_salary %>%
mutate(logsalary = (Salary+1)**2)
logmodel <- glm(JobSatisfaction ~ logsalary, data = has_salary2 , family = "gaussian")
glm.diag.plots(logmodel)
ggplot(has_salary, aes(Salary)) + geom_histogram(color="#667ea5", fill= "#9ec2ff")
# why are these people making so little?
# perhaps it's employment status:
low_salaries <- has_salary %>%
filter(Salary < 25000) %>%
group_by(EmploymentStatus)
ggplot(low_salaries, aes(Salary, fill=EmploymentStatus)) +
geom_histogram(color="green4", lwd=0.25)
by_country <- low_salaries %>%
add_count(Country) %>%
arrange(desc(n)) %>%
filter(n > 60)
head(unique(by_country$n), n=10)
ggplot(by_country, aes(Salary)) +
geom_histogram(fill = "#a55200", color= "#4f2700") +
facet_wrap(~Country)
america <- has_salary %>%
filter(Country == "United States")
head(america)
AmericaModel <- glm(JobSatisfaction ~ Salary, data = america , family = "gaussian")
glm.diag.plots(AmericaModel)
ggplot(america, aes(x=Salary, y = JobSatisfaction)) +
geom_jitter()
# middle america
middle_america <- america %>%
filter(Salary > 50000 && Salary < 150000) %>%
mutate(LogSalary = log(Salary))
MiddleAmericaModel <- glm(JobSatisfaction ~ Salary, data = middle_america , family = "gaussian")
ggplot(middle_america, aes(Salary, fill=EmploymentStatus)) +
geom_histogram(color="green4", lwd=0.25)
