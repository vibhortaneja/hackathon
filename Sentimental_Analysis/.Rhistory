library(ggplot2) # Libarary for Data visualization
library(readr) # library to read CSV file
library(syuzhet) #library for sentimental analysis
library(tm)
library(wordcloud)
library(dplyr)
library(tidytext)
library(tidyr)
library(igraph)
library(ggraph)
library(wordcloud2)
demonitization_data <-read.csv("C:\\Users\\VIBHOR TANEJA\\Desktop\\hackathon\\Sentimental_Analysis\\Data\\demonetization-tweets.csv")
demonitization_data <-read.csv("C:\\Users\\vibhor.5.taneja\\Desktop\\hackathon\\Sentimental_Analysis\\Data\\demonetization-tweets.csv")
demonitization_bigrams <- unnest_tokens(demonitization_data, input = text, output = bigram, token = "ngrams", n=2)
demonitization_bigrams %>% count(bigram, sort = TRUE)
demonitization_bigrams_separated <- demonitization_bigrams %>% separate(bigram, c("word1", "word2"), sep = " ")
demonitization_bigrams_filtered <- demonitization_bigrams_separated %>% filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word)
# new bigram counts:
demonitization_bigrams_counts <- demonitization_bigrams_filtered %>% count(word1, word2, sort = TRUE)
demonitization_bigrams_counts
AFINN <- get_sentiments("afinn")
not_words <- demonitization_bigrams_separated %>% filter(word1 == "not") %>% inner_join(AFINN, by = c(word2 = "word")) %>%
count(word2, score, sort = TRUE) %>% ungroup()
not_words %>% mutate(contribution = n * score) %>% arrange(desc(abs(contribution))) %>%
head(20) %>% mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(word2, n * score, fill = n * score > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by \"not\"") +
ylab("Sentiment score * number of occurrences") +
coord_flip()
######################### Sentiments according to predefined library ########################################
demonitization_data_text<-as.character(demonitization_data$text)
filter_text<-gsub("(RT|via)((?:\\b\\w*@\\w+)+)","",demonitization_data_text) #removing Retweets
filter_text<-gsub("http[^[:blank:]]+","",filter_text) #clean html links
filter_text<-gsub("@\\w+","",filter_text)# removing people names
filter_text<-gsub("[[:punct:]]"," ",filter_text)#removing punctuations
filter_text<-gsub("[^[:alnum:]]"," ",filter_text)#removing number (alphanumeric)
sentiment_twitter<-get_nrc_sentiment((filter_text)) #sentimental analysis
filter_text<-gsub("[^[:alnum:]]"," ",filter_text)#removing number (alphanumeric)
sentiment_twitter<-get_nrc_sentiment((filter_text)) #sentimental analysis
Sentimentscores<-data.frame(colSums(sentiment_twitter[,])) #used to classify sentiment scores
names(Sentimentscores)<-"Score"
SentimentScores<-cbind("sentiment"=rownames(Sentimentscores),Sentimentscores)
rownames(SentimentScores)<-NULL
#Ploting of data
ggplot(data=SentimentScores,aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Total sentiment based on scores")
library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function
library(tm)
library(wordcloud)
library(plyr)
library(lubridate)
library(syuzhet)
#Import the twitter data set
demonitization_data=read.csv('C:\\Users\\vibhor.5.taneja\\Desktop\\hackathon\\Sentimental_Analysis\\Data\\demonetization-tweets-2.csv',stringsAsFactors = FALSE)
options(warn=-1)
summary(demonitization_data)
demonitization_data$created_date=as.Date(demonitization_data$created,format='%Y-%m-%d %H:%M:%S')#convert created to date format
demonitization_data$hour = format(as.POSIXct(demonitization_data$created,format="%Y-%m-%d %H:%M:%S"),"%H")#Extract Hour from the date
demonitization_data$isRetweetNum=ifelse(demonitization_data$isRetweet==FALSE,0,1)#Numerical variable to indicate whether a tweet was retweet
demonitization_data$retweetedNum=ifelse(demonitization_data$retweeted==FALSE,0,1)#Total number of times a tweet was tetweeted
demonitization_data$tweet=c(1)#Additional column that will help us in summing up total tweets
options(repr.plot.width=6, repr.plot.height=4)
HourFrame=as.data.frame(table(demonitization_data$hour))
colnames(HourFrame)=c("Hour","TweetCount")
HourFrame$Hour=as.numeric(HourFrame$Hour)
y=ddply(demonitization_data, .(demonitization_data$hour), numcolwise(sum))
HourFrame$retweetedNum=y$isRetweetNum
ggplot(HourFrame,aes(x=Hour))+geom_line(aes(y = TweetCount, colour = "TotalTweets")) +
geom_line(aes(y = retweetedNum, colour = "Retweets"))
################################## device data ###########################################
devices=demonitization_data$statusSource
devices <- gsub("","", devices)
devices <- strsplit(devices, ">")
devices <- sapply(devices,function(x) ifelse(length(x) > 1, x[2], x[1]))
devices_source=as.data.frame(table(devices))
colnames(devices_source)=c("Device","TweetCount")
devices_source=devices_source[devices_source$TweetCount>50,]
devices_source=devices_source[order(-devices_source$TweetCount),]
ggplot(devices_source,aes(x=reorder(Device, -TweetCount),y=TweetCount,fill=TweetCount))+geom_bar(stat='identity') +coord_flip()
################ Most Popular users ######################################################
y=ddply(demonitization_data, .(screenName), numcolwise(sum))
popularUsers=y[,c("screenName","retweetCount","tweet")]
popularUsers=popularUsers[order(-popularUsers$retweetCount),]
popularUsers=head(popularUsers,n=10)
popularUsers
################ Most Replies ###############################################################
Replies=demonitization_data[is.na(demonitization_data$replyToSN)==FALSE,]
y=ddply(Replies, .(replyToSN), numcolwise(sum))
Replies=y[,c("replyToSN","tweet")]
Replies=Replies[order(-Replies$tweet),]
Replies=head(Replies,n=20)
colnames(Replies)=c("User","RepliesReceived")
Replies
###############Sentimental analysis #######################################################
some_txt<-gsub("(RT|via)((?:\\b\\w*@\\w+)+)","",demonitization_data$text)
some_txt<-gsub("http[^[:blank:]]+","",some_txt)
some_txt<-gsub("@\\w+","",some_txt)
some_txt<-gsub("[[:punct:]]"," ",some_txt)
some_txt<-gsub("[^[:alnum:]]"," ",some_txt)
tweetSentiment <- get_nrc_sentiment(some_txt)
barplot(
sort(colSums(prop.table(tweetSentiment[, 1:8]))),
#  horiz = TRUE,
cex.names = 0.7,
las = 1,
main = "Emotions in Tweets text", xlab="Percentage"
)
install.packages("npm")
install.packages("NLP")
install.packages("NLP")
library(RColorBrewer)
library(readr) # CSV file I/O, e.g. the read_csv function
library(tm)
library(stringr)
library(wordcloud)
library(RWeka)
install.packages("RWeka")
library(RWeka)
library(RSentiment)
library(SnowballC)
library(NLP)
install.packages("RSentiment")
library(RSentiment)
library(NLP)
library(RColorBrewer)
library(readr) # CSV file I/O, e.g. the read_csv function
library(tm)
library(stringr)
library(wordcloud)
library(RWeka)
library(RSentiment)
library(SnowballC)
tweet=read.csv('C:\\Users\\vibhor.5.taneja\\Desktop\\hackathon\\Sentimental_Analysis\\Data\\demonetization-tweets-3.csv',stringsAsFactors = FALSE)
tweet=read.csv('C:\\Users\\vibhor.5.taneja\\Desktop\\hackathon\\Sentimental_Analysis\\Data\\demonetization-tweets-3.csv',stringsAsFactors = FALSE)
text <- as.character(tweet$text)
set.seed(100)
sample <- sample(text, (length(text)))
corpus <- Corpus(VectorSource(list(sample)))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, stemDocument)
dtm_up <- DocumentTermMatrix(VCorpus(VectorSource(corpus[[1]]$content)))
freq_up <- colSums(as.matrix(dtm_up))
sentiments_up <- calculate_sentiment(names(freq_up))
sentiments_up <- cbind(sentiments_up, as.data.frame(freq_up))
sent_pos_up <- sentiments_up[sentiments_up$sentiment=='Positive',]
sent_neg_up <- sentiments_up[sentiments_up$sentiment=='Negative',]
cat("Negative sentiments: ",sum(sent_neg_up$freq_up)," Positive sentiments: ",sum(sent_pos_up$freq_up))
wordcloud(sent_pos_up$text,sent_pos_up$freq,min.freq=5,random.order=FALSE,colors=brewer.pal(6,"Dark2"))
wordcloud(sent_neg_up$text,sent_neg_up$freq,min.freq=5,random.order=FALSE,colors=brewer.pal(6,"Dark2"))
